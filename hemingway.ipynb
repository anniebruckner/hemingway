{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/annie'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use Terminal to download folder from GitHub\n",
    "#First change directory\n",
    "#cd '/Users/annie/Desktop/Northwestern/PREDICT_453/Hemingway/IOT'\n",
    "#Then download file by replacing 'tree/master' with trunk\n",
    "#svn checkout https://github.com/anniebruckner/trunk/hemingway\n",
    "# SOURCE: http://stackoverflow.com/questions/7106012/download-a-single-folder-or-directory-from-a-github-repo\n",
    "\n",
    "#get working directory\n",
    "import os\n",
    "os.getcwd() # Prints the working directory\n",
    "#option+return executes the code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/annie/Desktop/Northwestern/PREDICT_453/Hemingway/IOT/noTitles'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change working directory\n",
    "os.chdir('/Users/annie/Desktop/Northwestern/PREDICT_453/Hemingway/IOT/noTitles')\n",
    "os.getcwd()\n",
    "#This is just to tell Python where to find the .txt files--the .ipynb saves in '/Users/annie/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "#import codecs\n",
    "#from sklearn import feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib as mpl\n",
    "#import mpld3\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this data structure holds info about each IOT story\n",
    "## thank you to Marek Blat for the FileData Class\n",
    "class FileData(object):\n",
    "    name = \"\"\n",
    "    content = \"\"\n",
    "\n",
    "    def __init__(self, name, content):\n",
    "        self.name = name\n",
    "        self.content = content\n",
    "    \n",
    "    ## holds the names of each DSI\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    ## holds the raw content\n",
    "    def get_content(self):\n",
    "        return self.content\n",
    "    \n",
    "    ## returns content without stopwords. Probably does not scale well.\n",
    "    def get_content_filtered(self):\n",
    "        return ' '.join([word for word in self.content.split() if word not in stopwords])\n",
    "         \n",
    "\n",
    "## extract all file names from a path\n",
    "def get_files(path):\n",
    "    os.chdir(path)\n",
    "    files = [f for f in os.listdir()]\n",
    "    return files\n",
    "\n",
    "## keep only fileNames with IOT\n",
    "def get_txt_files(files):\n",
    "    txt_files = []\n",
    "    for fileName in files:\n",
    "        if \"IOT\" in fileName and \".txt\" in fileName:\n",
    "            txt_files.append(fileName)\n",
    "    return txt_files    \n",
    "\n",
    "## opens all the files that meet the criteria above\n",
    "def get_txt_file_data(path):\n",
    "    files = get_files(path)\n",
    "    txt_files = get_txt_files(files)\n",
    "    documents = []\n",
    "    for fileName in txt_files:\n",
    "        ## I added the ignore as some docs had chars that gave me an error\n",
    "        file = open(fileName, 'r', encoding='utf-8', errors = 'ignore')\n",
    "        content = file.read()\n",
    "        ## This uses the function defined above\n",
    "        fileData = FileData(fileName, content)\n",
    "        documents.append(fileData)\n",
    "    ## sort documents by DSI number\n",
    "    #documents = sorted(documents, key=lambda x: int(re.sub(r'(\\D)', \"\", x.get_name())[0]))\n",
    "    return documents\n",
    "\n",
    "\n",
    "path = \"/Users/annie/Desktop/Northwestern/PREDICT_453/Hemingway/IOT/noTitles\"\n",
    "docs = get_txt_file_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.FileData at 0x111584898>,\n",
       " <__main__.FileData at 0x104f8c8d0>,\n",
       " <__main__.FileData at 0x104fedf60>,\n",
       " <__main__.FileData at 0x111584b70>,\n",
       " <__main__.FileData at 0x111584ba8>,\n",
       " <__main__.FileData at 0x111584be0>,\n",
       " <__main__.FileData at 0x111584c18>,\n",
       " <__main__.FileData at 0x111584c50>,\n",
       " <__main__.FileData at 0x111584c88>,\n",
       " <__main__.FileData at 0x111584cc0>,\n",
       " <__main__.FileData at 0x111584cf8>,\n",
       " <__main__.FileData at 0x111584d30>,\n",
       " <__main__.FileData at 0x111584d68>,\n",
       " <__main__.FileData at 0x111584da0>,\n",
       " <__main__.FileData at 0x111584dd8>,\n",
       " <__main__.FileData at 0x111584e10>,\n",
       " <__main__.FileData at 0x111584e48>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 21656 items in vocab_frame\n",
      "(17, 34184)\n",
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: b'bull', b'old', b'old', b'man', b'he\\xe2\\x80\\x99d', b'horses',\n",
      "\n",
      "Cluster 0 IOTs:\n",
      "IOT_InOurTime.txt\n",
      "IOT_MyOldMan.txt\n",
      "IOT_OnTheQuaiAtSmyrna.txt\n",
      "IOT_TheRevolutionist.txt\n",
      "Cluster 1 words: b'nick', b'nick', b'george', b'water', b'trout', b'log',\n",
      "\n",
      "Cluster 1 IOTs:\n",
      "IOT_BigTwoHeartedRiverPartI.txt\n",
      "IOT_BigTwoHeartedRiverPartII.txt\n",
      "IOT_CrossCountrySnow.txt\n",
      "IOT_EndOfSomething.txt\n",
      "IOT_IndianCamp.txt\n",
      "IOT_TheBattler.txt\n",
      "IOT_TheDrAndTheDrWife.txt\n",
      "IOT_ThreeDayBlow.txt\n",
      "Cluster 2 words: b'krebs', b'elliot', b'luz', b'girl', b'cat', b'married',\n",
      "\n",
      "Cluster 2 IOTs:\n",
      "IOT_CatInTheRain.txt\n",
      "IOT_MrAndMrsElliot.txt\n",
      "IOT_SoldiersHome.txt\n",
      "IOT_VeryShortStory.txt\n",
      "Cluster 3 words: b'young', b'peduzzi', b'gentleman', b'young', b'marsala', b'wife',\n",
      "\n",
      "Cluster 3 IOTs:\n",
      "IOT_OutOfSeason.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and len(token) > 1: ## must be longer than 1\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and len(token) > 1: ## must be longer than 1\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens    \n",
    "\n",
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in range(len(docs)):\n",
    "    allwords_stemmed = tokenize_and_stem(docs[i].get_content_filtered()) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(docs[i].get_content_filtered())\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "\n",
    "## removes stopwords \n",
    "## filter_df = vocab_frame.index.isin(stopwords)\n",
    "## vocab_frame = vocab_frame[~filter_df]\n",
    "\n",
    "## Create tf-idf matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english'                                 \n",
    "                                   #,max_features=200000\n",
    "                                   #,min_df=0.2\n",
    "                                   ,max_df=0.8\n",
    "                                   ,use_idf=True\n",
    "                                   ,tokenizer=tokenize_and_stem\n",
    "                                   ,ngram_range=(1,3)\n",
    "                                   )\n",
    "\n",
    "## need to extract content and names from data structure\n",
    "doc_content = []\n",
    "for doc in docs:\n",
    "    doc_content.append(doc.get_content_filtered())\n",
    "\n",
    "doc_name = []\n",
    "for doc in docs:\n",
    "    doc_name.append(doc.get_name())\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(doc_content) #fit the vectorizer to content\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 4\n",
    "## runs cluster\n",
    "km = KMeans(n_clusters=num_clusters\n",
    "            ,random_state=42\n",
    "            ,max_iter = 1000) \n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "doc_dict = { 'IOT': doc_name, 'content': doc_content, 'cluster': clusters}\n",
    "frame = pd.DataFrame(doc_dict, index = [clusters] )\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    \n",
    "    print(\"Cluster %d IOTs:\" % i, end='')\n",
    "    print()\n",
    "    if type(frame.ix[i]['IOT']) is not str:\n",
    "        for dsi in list(frame.ix[i]['IOT']):\n",
    "            print(dsi)\n",
    "    else:\n",
    "        print(str(frame.ix[i]['IOT']))\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix\n",
    "                ,orientation=\"right\"\n",
    "                ,labels=doc_name)\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "##plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
